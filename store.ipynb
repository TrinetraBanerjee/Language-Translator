{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TrinetraBanerjee/Language-Translator/blob/main/store.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for dubbibg mutli"
      ],
      "metadata": {
        "id": "KRjkYHIbHyvp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWa0OuOfHNGz"
      },
      "outputs": [],
      "source": [
        "!pip install -qq https://github.com/pyannote/pyannote-audio/archive/refs/heads/develop.zip\n",
        "!pip install -qq ipython==7.34.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition\n",
        "!pip install pocketsphinx"
      ],
      "metadata": {
        "id": "hnED5-hnHU8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "\n",
        "video = VideoFileClip(\"/content/WhatsApp Video 2024-10-12 at 21.55.20 (1).mp4\")\n",
        "\n",
        "audio_path = \"/content/Train.wav\"\n",
        "video.audio.write_audiofile(audio_path)\n"
      ],
      "metadata": {
        "id": "zTXDeUePHXMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyannote.audio import Pipeline\n",
        "\n",
        "\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", use_auth_token=\"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\")\n",
        "\n",
        "\n",
        "diarization = pipeline(audio_path, num_speakers=2)\n",
        "\n",
        "\n",
        "print(diarization)\n",
        "with open(\"sample.rttm\", \"w\") as rttm:\n",
        "    diarization.write_rttm(rttm)\n"
      ],
      "metadata": {
        "id": "M3C2kMTiHaaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/openai/whisper.git\n"
      ],
      "metadata": {
        "id": "2eY6Kn79Hedx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pydub"
      ],
      "metadata": {
        "id": "YqV7aeVCHfZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gtts"
      ],
      "metadata": {
        "id": "M_MkMwVjHjyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import whisper\n",
        "from gtts import gTTS\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "def extract_text_from_audio(audio_path, start, end):\n",
        "    \"\"\"Extract text from audio between start and end times using Whisper.\"\"\"\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    audio_segment = audio[start * 1000:end * 1000]  # Convert seconds to milliseconds\n",
        "    temp_audio_path = \"temp_segment.wav\"\n",
        "    audio_segment.export(temp_audio_path, format=\"wav\")\n",
        "\n",
        "    # Load Whisper model (adjust to use your model size as needed)\n",
        "    model = whisper.load_model(\"base\")\n",
        "\n",
        "    # Transcribe the audio using Whisper\n",
        "    result = model.transcribe(temp_audio_path)\n",
        "    text = result['text'].strip()\n",
        "\n",
        "    # Return None if no text was transcribed\n",
        "    return text if text else None\n",
        "\n",
        "def pitch_shift(sound, semitones):\n",
        "    \"\"\"Shift the pitch of the sound by a given number of semitones.\"\"\"\n",
        "    return sound._spawn(sound.raw_data, overrides={\n",
        "        \"frame_rate\": int(sound.frame_rate * (2.0 ** (semitones / 12.0)))\n",
        "    }).set_frame_rate(sound.frame_rate)\n",
        "\n",
        "def text_to_speech(text, speaker_id):\n",
        "    \"\"\"Convert text to speech, adjust the pitch, and return an AudioSegment.\"\"\"\n",
        "    # Generate speech using gTTS\n",
        "    tts = gTTS(text)\n",
        "    temp_audio_path = f\"temp_speech_{speaker_id}.mp3\"\n",
        "    tts.save(temp_audio_path)\n",
        "\n",
        "    # Load the speech audio\n",
        "    sound = AudioSegment.from_mp3(temp_audio_path)\n",
        "\n",
        "    # Adjust pitch: Speaker 0 is lower pitch, Speaker 1 is higher pitch\n",
        "    if speaker_id == \"SPEAKER_00\":\n",
        "        sound = pitch_shift(sound, 2)  # Lower pitch by 2 semitones\n",
        "    elif speaker_id == \"SPEAKER_01\":\n",
        "        sound = pitch_shift(sound, -3)  # Raise pitch by 3 semitones\n",
        "\n",
        "    os.remove(temp_audio_path)  # Clean up temporary file\n",
        "    return sound\n",
        "\n",
        "def rttm_dataframe(rttm_file_path, audio_path):\n",
        "    \"\"\"Create a DataFrame from an RTTM file and extract utterances.\"\"\"\n",
        "    col = [\"Type\", \"File_Id\", \"Channel\", \"Start Time\", \"Duration\", \"Speaker\", \"Unused1\", \"Unused2\", \"Utterance\"]\n",
        "    data = []\n",
        "\n",
        "    try:\n",
        "        with open(rttm_file_path, 'r') as rttm:\n",
        "            lines = rttm.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip().split()\n",
        "            if len(line) >= len(col) - 1:  # Ensure line has enough columns\n",
        "                start_time = float(line[3])  # Start Time\n",
        "                duration = float(line[4])    # Duration\n",
        "                end_time = start_time + duration\n",
        "\n",
        "                # Extract text from the audio\n",
        "                text = extract_text_from_audio(audio_path, start_time, end_time)\n",
        "\n",
        "                # Skip the row if no text was extracted\n",
        "                if text is None:\n",
        "                    continue\n",
        "\n",
        "                # Add the extracted text for the 'Utterance' column\n",
        "                data.append(line[:len(col) - 1] + [text])\n",
        "\n",
        "        df = pd.DataFrame(data, columns=col)\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {rttm_file_path} does not exist.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "def combine_speech_from_rttm(rttm_file_path, audio_path, output_combined_path, silence_duration=1):\n",
        "    \"\"\"Combine generated speech from the RTTM file into a single audio file, with optional silence between utterances.\"\"\"\n",
        "    # Load the RTTM data\n",
        "    df = rttm_dataframe(rttm_file_path, audio_path)\n",
        "\n",
        "    if df is None:\n",
        "        print(\"DataFrame is None. Check the RTTM file.\")\n",
        "        return\n",
        "\n",
        "    df[\"Start Time\"] = pd.to_numeric(df[\"Start Time\"])\n",
        "    df[\"Duration\"] = pd.to_numeric(df[\"Duration\"])\n",
        "\n",
        "    # Create an empty AudioSegment to store the combined audio\n",
        "    combined_audio = AudioSegment.empty()\n",
        "\n",
        "    # Initialize a variable to keep track of the end time of the last utterance\n",
        "    last_end_time = 0\n",
        "\n",
        "    # Iterate over each row, generate TTS, and append it to the combined audio\n",
        "    for idx, row in df.iterrows():\n",
        "        utterance = row[\"Utterance\"]\n",
        "        speaker = row[\"Unused2\"]\n",
        "\n",
        "        # Generate the TTS audio for the utterance\n",
        "        tts_audio = text_to_speech(utterance, speaker)\n",
        "\n",
        "        # Add silence between the last utterance and the new one (if required)\n",
        "        if last_end_time > 0:\n",
        "            silence = AudioSegment.silent(duration=silence_duration * 1000)  # silence_duration in seconds\n",
        "            combined_audio += silence\n",
        "\n",
        "        # Append the generated TTS audio to the combined audio\n",
        "        combined_audio += tts_audio\n",
        "\n",
        "        # Update the end time of the current utterance\n",
        "        last_end_time += len(tts_audio)\n",
        "\n",
        "    # Export the combined audio to a file\n",
        "    combined_audio.export(output_combined_path, format=\"wav\")\n",
        "    print(f\"Combined speech file saved as {output_combined_path}\")\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"/content/Train.wav\"\n",
        "rttm_file_path = \"sample.rttm\"\n",
        "output_combined_path = \"combined_speech.wav\"\n",
        "\n",
        "# Combine the speech and save the result with no silence between utterances\n",
        "combine_speech_from_rttm(rttm_file_path, audio_path, output_combined_path, silence_duration=1)\n"
      ],
      "metadata": {
        "id": "xTfmctMcHp9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the final audio\n",
        "file = \"/content/combined_speech.wav\"\n",
        "# Assign the combined audio from previous step to final_audio\n",
        "# Assuming the previous cell/function produced combined_audio\n",
        "final_audio = AudioSegment.from_wav(file)  # Load the combined audio\n",
        "final_audio.export(file, format=\"wav\")  # Now you can export\n",
        "\n",
        "# In Google Colab, display the audio file\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# Display the audio in Colab\n",
        "display(Audio(file))"
      ],
      "metadata": {
        "id": "HQW8ECuyHrEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code with text summarization\n"
      ],
      "metadata": {
        "id": "dTOi5p-GQ7PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import whisper\n",
        "from gtts import gTTS\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "def extract_text_from_audio(audio_path, start, end):\n",
        "    \"\"\"Extract text from audio between start and end times using Whisper.\"\"\"\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    audio_segment = audio[start * 1000:end * 1000]  # Convert seconds to milliseconds\n",
        "    temp_audio_path = \"temp_segment.wav\"\n",
        "    audio_segment.export(temp_audio_path, format=\"wav\")\n",
        "\n",
        "    # Load Whisper model (adjust to use your model size as needed)\n",
        "    model = whisper.load_model(\"base\")\n",
        "\n",
        "    # Transcribe the audio using Whisper\n",
        "    result = model.transcribe(temp_audio_path)\n",
        "    text = result['text'].strip()\n",
        "\n",
        "    # Return None if no text was transcribed\n",
        "    return text if text else None\n",
        "\n",
        "def pitch_shift(sound, semitones):\n",
        "    \"\"\"Shift the pitch of the sound by a given number of semitones.\"\"\"\n",
        "    return sound._spawn(sound.raw_data, overrides={\n",
        "        \"frame_rate\": int(sound.frame_rate * (2.0 ** (semitones / 12.0)))\n",
        "    }).set_frame_rate(sound.frame_rate)\n",
        "\n",
        "def text_to_speech(text, speaker_id):\n",
        "    \"\"\"Convert text to speech, adjust the pitch, and return an AudioSegment.\"\"\"\n",
        "    # Generate speech using gTTS\n",
        "    tts = gTTS(text)\n",
        "    temp_audio_path = f\"temp_speech_{speaker_id}.mp3\"\n",
        "    tts.save(temp_audio_path)\n",
        "\n",
        "    # Load the speech audio\n",
        "    sound = AudioSegment.from_mp3(temp_audio_path)\n",
        "\n",
        "    # Adjust pitch: Speaker 0 is lower pitch, Speaker 1 is higher pitch\n",
        "    if speaker_id == \"SPEAKER_00\":\n",
        "        sound = pitch_shift(sound, 2)  # Lower pitch by 2 semitones\n",
        "    elif speaker_id == \"SPEAKER_01\":\n",
        "        sound = pitch_shift(sound, -3)  # Raise pitch by 3 semitones\n",
        "\n",
        "    os.remove(temp_audio_path)  # Clean up temporary file\n",
        "    return sound\n",
        "\n",
        "def rttm_dataframe(rttm_file_path, audio_path):\n",
        "    \"\"\"Create a DataFrame from an RTTM file and extract utterances.\"\"\"\n",
        "    col = [\"Type\", \"File_Id\", \"Channel\", \"Start Time\", \"Duration\", \"Speaker\", \"Unused1\", \"Unused2\", \"Utterance\"]\n",
        "    data = []\n",
        "\n",
        "    try:\n",
        "        with open(rttm_file_path, 'r') as rttm:\n",
        "            lines = rttm.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip().split()\n",
        "            if len(line) >= len(col) - 1:  # Ensure line has enough columns\n",
        "                start_time = float(line[3])  # Start Time\n",
        "                duration = float(line[4])    # Duration\n",
        "                end_time = start_time + duration\n",
        "\n",
        "                # Extract text from the audio\n",
        "                text = extract_text_from_audio(audio_path, start_time, end_time)\n",
        "\n",
        "                # Skip the row if no text was extracted\n",
        "                if text is None:\n",
        "                    continue\n",
        "\n",
        "                # Add the extracted text for the 'Utterance' column\n",
        "                data.append(line[:len(col) - 1] + [text])\n",
        "\n",
        "        df = pd.DataFrame(data, columns=col)\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {rttm_file_path} does not exist.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "def summarize_and_edit_utterances(df):\n",
        "    \"\"\"Show each speaker's utterances one by one, allowing the user to modify or delete each utterance individually.\"\"\"\n",
        "\n",
        "    # Iterate over each unique speaker\n",
        "    for speaker in df[\"Unused2\"].unique():\n",
        "        # Get all utterances for the current speaker\n",
        "        speaker_utterances = df[df[\"Unused2\"] == speaker][\"Utterance\"].tolist()\n",
        "\n",
        "        print(f\"\\nEditing utterances for {speaker}:\")\n",
        "\n",
        "        # Iterate through each utterance and prompt the user for editing\n",
        "        for i, utterance in enumerate(speaker_utterances):\n",
        "            print(f\"Original Utterance {i + 1}: {utterance}\")\n",
        "            user_input = input(\"Edit this utterance, type 'delete' to remove it, or press Enter to keep as is:\\n\")\n",
        "\n",
        "            if user_input.lower() == 'delete':\n",
        "                # Delete the specific utterance from the DataFrame\n",
        "                df = df.drop(df[(df[\"Unused2\"] == speaker) & (df[\"Utterance\"] == utterance)].index)\n",
        "                print(\"Utterance deleted.\")\n",
        "            elif user_input:  # If user_input is not empty, update the utterance\n",
        "                # Update the specific utterance in the DataFrame\n",
        "                df.loc[(df[\"Unused2\"] == speaker) & (df[\"Utterance\"] == utterance), \"Utterance\"] = user_input\n",
        "                print(\"Utterance updated.\")\n",
        "\n",
        "    # Reset index after deletions\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def combine_speech_from_rttm(rttm_file_path, audio_path, output_combined_path, silence_duration=1):\n",
        "    \"\"\"Combine generated speech from the RTTM file into a single audio file, with optional silence between utterances.\"\"\"\n",
        "    # Load the RTTM data\n",
        "    df = rttm_dataframe(rttm_file_path, audio_path)\n",
        "\n",
        "    if df is None:\n",
        "        print(\"DataFrame is None. Check the RTTM file.\")\n",
        "        return\n",
        "\n",
        "    df[\"Start Time\"] = pd.to_numeric(df[\"Start Time\"])\n",
        "    df[\"Duration\"] = pd.to_numeric(df[\"Duration\"])\n",
        "\n",
        "    # Summarize and allow for edits\n",
        "    df = summarize_and_edit_utterances(df)\n",
        "\n",
        "    # Create an empty AudioSegment to store the combined audio\n",
        "    combined_audio = AudioSegment.empty()\n",
        "\n",
        "    # Initialize a variable to keep track of the end time of the last utterance\n",
        "    last_end_time = 0\n",
        "\n",
        "    # Iterate over each row, generate TTS, and append it to the combined audio\n",
        "    for idx, row in df.iterrows():\n",
        "        utterance = row[\"Utterance\"]\n",
        "        speaker = row[\"Unused2\"]\n",
        "\n",
        "        # Generate the TTS audio for the utterance\n",
        "        tts_audio = text_to_speech(utterance, speaker)\n",
        "\n",
        "        # Add silence between the last utterance and the new one (if required)\n",
        "        if last_end_time > 0:\n",
        "            silence = AudioSegment.silent(duration=silence_duration * 1000)  # silence_duration in seconds\n",
        "            combined_audio += silence\n",
        "\n",
        "        # Append the generated TTS audio to the combined audio\n",
        "        combined_audio += tts_audio\n",
        "\n",
        "        # Update the end time of the current utterance\n",
        "        last_end_time += len(tts_audio)\n",
        "\n",
        "    # Export the combined audio to a file\n",
        "    combined_audio.export(output_combined_path, format=\"wav\")\n",
        "    print(f\"Combined speech file saved as {output_combined_path}\")\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"/content/Train.wav\"\n",
        "rttm_file_path = \"sample.rttm\"\n",
        "output_combined_path = \"combined_speech.wav\"\n",
        "\n",
        "# Combine the speech and save the result with no silence between utterances\n",
        "combine_speech_from_rttm(rttm_file_path, audio_path, output_combined_path, silence_duration=1)\n"
      ],
      "metadata": {
        "id": "1mrGux8PQ5GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for 4 speakers\n"
      ],
      "metadata": {
        "id": "8AgPEHCE_VbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import whisper\n",
        "from gtts import gTTS\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "def extract_text_from_audio(audio_path, start, end):\n",
        "    \"\"\"Extract text from audio between start and end times using Whisper.\"\"\"\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    audio_segment = audio[start * 1000:end * 1000]  # Convert seconds to milliseconds\n",
        "    temp_audio_path = \"temp_segment.wav\"\n",
        "    audio_segment.export(temp_audio_path, format=\"wav\")\n",
        "\n",
        "    # Load Whisper model (adjust to use your model size as needed)\n",
        "    model = whisper.load_model(\"base\")\n",
        "\n",
        "    # Transcribe the audio using Whisper\n",
        "    result = model.transcribe(temp_audio_path)\n",
        "    text = result['text'].strip()\n",
        "\n",
        "    # Return None if no text was transcribed\n",
        "    return text if text else None\n",
        "\n",
        "def pitch_shift(sound, semitones):\n",
        "    \"\"\"Shift the pitch of the sound by a given number of semitones.\"\"\"\n",
        "    return sound._spawn(sound.raw_data, overrides={\n",
        "        \"frame_rate\": int(sound.frame_rate * (2.0 ** (semitones / 12.0)))\n",
        "    }).set_frame_rate(sound.frame_rate)\n",
        "\n",
        "def text_to_speech(text, speaker_id):\n",
        "    \"\"\"Convert text to speech, adjust the pitch, and return an AudioSegment.\"\"\"\n",
        "    # Generate speech using gTTS\n",
        "    tts = gTTS(text)\n",
        "    temp_audio_path = f\"temp_speech_{speaker_id}.mp3\"\n",
        "    tts.save(temp_audio_path)\n",
        "\n",
        "    # Load the speech audio\n",
        "    sound = AudioSegment.from_mp3(temp_audio_path)\n",
        "\n",
        "    # Adjust pitch: Speaker 0 is lower pitch, Speaker 1 is higher pitch\n",
        "    if speaker_id == \"SPEAKER_00\":\n",
        "        sound = pitch_shift(sound, 2)  # Lower pitch by 2 semitones\n",
        "    elif speaker_id == \"SPEAKER_01\":\n",
        "        sound = pitch_shift(sound, -3)  # Raise pitch by 3 semitones\n",
        "    elif speaker_id == \"SPEAKER_02\":\n",
        "        sound = pitch_shift(sound, -3)\n",
        "    elif speaker_id == \"SPEAKER_03\":\n",
        "        sound = pitch_shift(sound, -3)\n",
        "    os.remove(temp_audio_path)  # Clean up temporary file\n",
        "    return sound\n",
        "\n",
        "def rttm_dataframe(rttm_file_path, audio_path):\n",
        "    \"\"\"Create a DataFrame from an RTTM file and extract utterances.\"\"\"\n",
        "    col = [\"Type\", \"File_Id\", \"Channel\", \"Start Time\", \"Duration\", \"Speaker\", \"Unused1\", \"Unused2\", \"Utterance\"]\n",
        "    data = []\n",
        "\n",
        "    try:\n",
        "        with open(rttm_file_path, 'r') as rttm:\n",
        "            lines = rttm.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip().split()\n",
        "            if len(line) >= len(col) - 1:  # Ensure line has enough columns\n",
        "                start_time = float(line[3])  # Start Time\n",
        "                duration = float(line[4])    # Duration\n",
        "                end_time = start_time + duration\n",
        "\n",
        "                # Extract text from the audio\n",
        "                text = extract_text_from_audio(audio_path, start_time, end_time)\n",
        "\n",
        "                # Skip the row if no text was extracted\n",
        "                if text is None:\n",
        "                    continue\n",
        "\n",
        "                # Add the extracted text for the 'Utterance' column\n",
        "                data.append(line[:len(col) - 1] + [text])\n",
        "\n",
        "        df = pd.DataFrame(data, columns=col)\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {rttm_file_path} does not exist.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "def summarize_and_edit_utterances(df):\n",
        "    \"\"\"Show each speaker's utterances one by one, allowing the user to modify or delete each utterance individually.\"\"\"\n",
        "\n",
        "    # Iterate over each unique speaker\n",
        "    for speaker in df[\"Unused2\"].unique():\n",
        "        # Get all utterances for the current speaker\n",
        "        speaker_utterances = df[df[\"Unused2\"] == speaker][\"Utterance\"].tolist()\n",
        "\n",
        "        print(f\"\\nEditing utterances for {speaker}:\")\n",
        "\n",
        "        # Iterate through each utterance and prompt the user for editing\n",
        "        for i, utterance in enumerate(speaker_utterances):\n",
        "            print(f\"Original Utterance {i + 1}: {utterance}\")\n",
        "            user_input = input(\"Edit this utterance, type 'delete' to remove it, or press Enter to keep as is:\\n\")\n",
        "\n",
        "            if user_input.lower() == 'delete':\n",
        "                # Delete the specific utterance from the DataFrame\n",
        "                df = df.drop(df[(df[\"Unused2\"] == speaker) & (df[\"Utterance\"] == utterance)].index)\n",
        "                print(\"Utterance deleted.\")\n",
        "            elif user_input:  # If user_input is not empty, update the utterance\n",
        "                # Update the specific utterance in the DataFrame\n",
        "                df.loc[(df[\"Unused2\"] == speaker) & (df[\"Utterance\"] == utterance), \"Utterance\"] = user_input\n",
        "                print(\"Utterance updated.\")\n",
        "\n",
        "    # Reset index after deletions\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def combine_speech_from_rttm(rttm_file_path, audio_path, output_combined_path, silence_duration=1):\n",
        "    \"\"\"Combine generated speech from the RTTM file into a single audio file, with optional silence between utterances.\"\"\"\n",
        "    # Load the RTTM data\n",
        "    df = rttm_dataframe(rttm_file_path, audio_path)\n",
        "\n",
        "    if df is None:\n",
        "        print(\"DataFrame is None. Check the RTTM file.\")\n",
        "        return\n",
        "\n",
        "    df[\"Start Time\"] = pd.to_numeric(df[\"Start Time\"])\n",
        "    df[\"Duration\"] = pd.to_numeric(df[\"Duration\"])\n",
        "\n",
        "    # Summarize and allow for edits\n",
        "    df = summarize_and_edit_utterances(df)\n",
        "\n",
        "    # Create an empty AudioSegment to store the combined audio\n",
        "    combined_audio = AudioSegment.empty()\n",
        "\n",
        "    # Initialize a variable to keep track of the end time of the last utterance\n",
        "    last_end_time = 0\n",
        "\n",
        "    # Iterate over each row, generate TTS, and append it to the combined audio\n",
        "    for idx, row in df.iterrows():\n",
        "        utterance = row[\"Utterance\"]\n",
        "        speaker = row[\"Unused2\"]\n",
        "\n",
        "        # Generate the TTS audio for the utterance\n",
        "        tts_audio = text_to_speech(utterance, speaker)\n",
        "\n",
        "        # Add silence between the last utterance and the new one (if required)\n",
        "        if last_end_time > 0:\n",
        "            silence = AudioSegment.silent(duration=silence_duration * 1000)  # silence_duration in seconds\n",
        "            combined_audio += silence\n",
        "\n",
        "        # Append the generated TTS audio to the combined audio\n",
        "        combined_audio += tts_audio\n",
        "\n",
        "        # Update the end time of the current utterance\n",
        "        last_end_time += len(tts_audio)\n",
        "\n",
        "    # Export the combined audio to a file\n",
        "    combined_audio.export(output_combined_path, format=\"wav\")\n",
        "    print(f\"Combined speech file saved as {output_combined_path}\")\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"/content/Train.wav\"\n",
        "rttm_file_path = \"sample.rttm\"\n",
        "output_combined_path = \"combined_speech.wav\"\n",
        "\n",
        "# Combine the speech and save the result with no silence between utterances\n",
        "combine_speech_from_rttm(rttm_file_path, audio_path, output_combined_path, silence_duration=1)\n"
      ],
      "metadata": {
        "id": "8GA81gY0_T6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "import pandas as pd\n",
        "import whisper\n",
        "from gtts import gTTS\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "# Set the target language for translation\n",
        "target_language = \"Hindi\"  # Example target language\n",
        "\n",
        "# Mapping between full language names and ISO 639-1 codes\n",
        "language_mapping = {\n",
        "    'Bengali': 'bn',\n",
        "    'Hindi': 'hi',\n",
        "    'English': 'en',\n",
        "    'Spanish': 'es',\n",
        "    'French': 'fr',\n",
        "    'German': 'de',\n",
        "    'Italian': 'it',\n",
        "    'Portuguese': 'pt',\n",
        "    'Polish': 'pl',\n",
        "    'Turkish': 'tr',\n",
        "    'Russian': 'ru',\n",
        "    'Dutch': 'nl',\n",
        "    'Czech': 'cs',\n",
        "    'Arabic': 'ar',\n",
        "    'Chinese (Simplified)': 'zh-cn'\n",
        "}\n",
        "target = language_mapping[target_language]\n",
        "\n",
        "def extract_text_from_audio(audio_path, start, end):\n",
        "    \"\"\"Extract text from audio between start and end times using Whisper.\"\"\"\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    audio_segment = audio[start * 1000:end * 1000]\n",
        "    temp_audio_path = \"temp_segment.wav\"\n",
        "    audio_segment.export(temp_audio_path, format=\"wav\")\n",
        "\n",
        "    # Load Whisper model (adjust to use your model size as needed)\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(temp_audio_path)\n",
        "    text = result['text'].strip()\n",
        "    os.remove(temp_audio_path)  # Clean up temp file\n",
        "    return text if text else None\n",
        "\n",
        "def pitch_shift(sound, semitones):\n",
        "    \"\"\"Shift the pitch of the sound by a given number of semitones.\"\"\"\n",
        "    return sound._spawn(sound.raw_data, overrides={\n",
        "        \"frame_rate\": int(sound.frame_rate * (2.0 ** (semitones / 12.0)))\n",
        "    }).set_frame_rate(sound.frame_rate)\n",
        "\n",
        "def summarize_and_edit_utterances(df):\n",
        "    \"\"\"Show each speaker's utterances one by one, allowing the user to modify or delete each utterance individually.\"\"\"\n",
        "    for speaker in df[\"Unused2\"].unique():\n",
        "        speaker_utterances = df[df[\"Unused2\"] == speaker][\"Utterance\"].tolist()\n",
        "        print(f\"\\nEditing utterances for {speaker}:\")\n",
        "\n",
        "        for i, utterance in enumerate(speaker_utterances):\n",
        "            print(f\"Original Utterance {i + 1}: {utterance}\")\n",
        "            user_input = input(\"Edit this utterance, type 'delete' to remove it, or press Enter to keep as is:\\n\")\n",
        "\n",
        "            if user_input.lower() == 'delete':\n",
        "                df = df.drop(df[(df[\"Unused2\"] == speaker) & (df[\"Utterance\"] == utterance)].index)\n",
        "                print(\"Utterance deleted.\")\n",
        "            elif user_input:\n",
        "                df.loc[(df[\"Unused2\"] == speaker) & (df[\"Utterance\"] == utterance), \"Utterance\"] = user_input\n",
        "                print(\"Utterance updated.\")\n",
        "\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def translate_text(text, target_language_code):\n",
        "    \"\"\"Translate text into the target language and return both original and translated text.\"\"\"\n",
        "    try:\n",
        "        translated_text = GoogleTranslator(source='auto', target=target_language_code).translate(text)\n",
        "        return translated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error: {e}\")\n",
        "        return text  # Return the original text if translation fails\n",
        "\n",
        "def text_to_speech(text, speaker_id):\n",
        "    \"\"\"Convert text to speech, adjust the pitch, and return an AudioSegment.\"\"\"\n",
        "    tts = gTTS(text, lang=target)\n",
        "    temp_audio_path = f\"temp_speech_{speaker_id}.mp3\"\n",
        "    tts.save(temp_audio_path)\n",
        "    sound = AudioSegment.from_mp3(temp_audio_path)\n",
        "\n",
        "    # Adjust pitch based on speaker\n",
        "    pitch_adjustments = {\n",
        "        \"SPEAKER_00\": 2,\n",
        "        \"SPEAKER_01\": -3,\n",
        "        \"SPEAKER_02\": -3,\n",
        "        \"SPEAKER_03\": -3\n",
        "    }\n",
        "    semitones = pitch_adjustments.get(speaker_id, 0)\n",
        "    sound = pitch_shift(sound, semitones)\n",
        "\n",
        "    os.remove(temp_audio_path)  # Clean up temporary file\n",
        "    return sound\n",
        "\n",
        "def rttm_dataframe(rttm_file_path, audio_path):\n",
        "    \"\"\"Create a DataFrame from an RTTM file and extract utterances.\"\"\"\n",
        "    col = [\"Type\", \"File_Id\", \"Channel\", \"Start Time\", \"Duration\", \"Speaker\", \"Unused1\", \"Unused2\", \"Utterance\"]\n",
        "    data = []\n",
        "\n",
        "    try:\n",
        "        with open(rttm_file_path, 'r') as rttm:\n",
        "            lines = rttm.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip().split()\n",
        "            if len(line) >= len(col) - 1:\n",
        "                start_time = float(line[3])\n",
        "                duration = float(line[4])\n",
        "                end_time = start_time + duration\n",
        "                text = extract_text_from_audio(audio_path, start_time, end_time)\n",
        "\n",
        "                if text is None:\n",
        "                    continue\n",
        "\n",
        "                data.append(line[:len(col) - 1] + [text])\n",
        "\n",
        "        df = pd.DataFrame(data, columns=col)\n",
        "        df = summarize_and_edit_utterances(df)  # Summarize and edit first\n",
        "\n",
        "        # Translate each edited utterance\n",
        "        for idx, row in df.iterrows():\n",
        "            edited_text = row[\"Utterance\"]\n",
        "            translated_text = translate_text(edited_text, target)\n",
        "            df.at[idx, \"Utterance\"] = translated_text\n",
        "\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {rttm_file_path} does not exist.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "def combine_speech_from_rttm(rttm_file_path, audio_path, output_combined_path, silence_duration=0.8):\n",
        "    \"\"\"Combine generated speech from the RTTM file into a single audio file, with optional silence between utterances.\"\"\"\n",
        "    df = rttm_dataframe(rttm_file_path, audio_path)\n",
        "    if df is None:\n",
        "        print(\"DataFrame is None. Check the RTTM file.\")\n",
        "        return\n",
        "\n",
        "    df[\"Start Time\"] = pd.to_numeric(df[\"Start Time\"])\n",
        "    df[\"Duration\"] = pd.to_numeric(df[\"Duration\"])\n",
        "\n",
        "    combined_audio = AudioSegment.empty()\n",
        "    last_end_time = 0\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        utterance = row[\"Utterance\"]\n",
        "        speaker = row[\"Unused2\"]\n",
        "        tts_audio = text_to_speech(utterance, speaker)\n",
        "\n",
        "        if last_end_time > 0:\n",
        "            silence = AudioSegment.silent(duration=silence_duration * 1000)\n",
        "            combined_audio += silence\n",
        "\n",
        "        combined_audio += tts_audio\n",
        "        last_end_time += len(tts_audio)\n",
        "\n",
        "    combined_audio.export(output_combined_path, format=\"wav\")\n",
        "    print(f\"Combined speech file saved as {output_combined_path}\")\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"/content/Train.wav\"\n",
        "rttm_file_path = \"sample.rttm\"\n",
        "output_combined_path = \"combined_speech.wav\"\n",
        "combine_speech_from_rttm(rttm_file_path, audio_path, output_combined_path, silence_duration=0.7)\n"
      ],
      "metadata": {
        "id": "Xs9NqRzfHcta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}